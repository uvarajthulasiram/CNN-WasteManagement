{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lf5lYawIw8tE"
   },
   "source": [
    "# **Waste Material Segregation for Improving Waste Management**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NY1InIbkw80B"
   },
   "source": [
    "## **Objective**\n",
    "\n",
    "The objective of this project is to implement an effective waste material segregation system using convolutional neural networks (CNNs) that categorises waste into distinct groups. This process enhances recycling efficiency, minimises environmental pollution, and promotes sustainable waste management practices.\n",
    "\n",
    "The key goals are:\n",
    "\n",
    "* Accurately classify waste materials into categories like cardboard, glass, paper, and plastic.\n",
    "* Improve waste segregation efficiency to support recycling and reduce landfill waste.\n",
    "* Understand the properties of different waste materials to optimise sorting methods for sustainability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZGTCfyUxalZ"
   },
   "source": [
    "## **Data Understanding**\n",
    "\n",
    "The Dataset consists of images of some common waste materials.\n",
    "\n",
    "1. Food Waste\n",
    "2. Metal\n",
    "3. Paper\n",
    "4. Plastic\n",
    "5. Other\n",
    "6. Cardboard\n",
    "7. Glass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZJtmMnzQjAr"
   },
   "source": [
    "**Data Description**\n",
    "\n",
    "* The dataset consists of multiple folders, each representing a specific class, such as `Cardboard`, `Food_Waste`, and `Metal`.\n",
    "* Within each folder, there are images of objects that belong to that category.\n",
    "* However, these items are not further subcategorised. <br> For instance, the `Food_Waste` folder may contain images of items like coffee grounds, teabags, and fruit peels, without explicitly stating that they are actually coffee grounds or teabags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UBFt43WDzWSJ"
   },
   "source": [
    "## **1. Load the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dfy0rjJ1yzFl"
   },
   "source": [
    "Load and unzip the dataset zip file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N35LLuWXzUQH"
   },
   "source": [
    "**Import Necessary Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DmZo7m1-J_Ou"
   },
   "outputs": [],
   "source": [
    "# Recommended versions:\n",
    "\n",
    "# numpy version: 1.26.4\n",
    "# pandas version: 2.2.2\n",
    "# seaborn version: 0.13.2\n",
    "# matplotlib version: 3.10.0\n",
    "# PIL version: 11.1.0\n",
    "# tensorflow version: 2.18.0\n",
    "# keras version: 3.8.0\n",
    "# sklearn version: 1.6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dzM50pygphUe"
   },
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import sklearn\n",
    "import zipfile\n",
    "import os\n",
    "import warnings \n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Pandas by default doesn't display all the columns in the dataframe\n",
    "# As we're going to work on a large dataset, the following setting will help read data from all the columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Disable scientific notation\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "\n",
    "# For some of the columns we may have to see the data from all rows\n",
    "# Eg: Categorical columns.\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Set themes, styles and color palette for seaborn charts\n",
    "sns.set_theme(style='darkgrid', context='poster')\n",
    "sns.set_palette(palette='pastel', n_colors=10)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 4)\n",
    "plt.rcParams[\"axes.titlesize\"] = 14 \n",
    "plt.rcParams[\"axes.labelsize\"] = 12 \n",
    "plt.rcParams[\"xtick.labelsize\"] = 10 \n",
    "plt.rcParams[\"ytick.labelsize\"] = 10 \n",
    "\n",
    "# Print version numbers\n",
    "print('Numpy version:      ' + np.__version__)\n",
    "print('Pandas version:     ' + pd.__version__)\n",
    "print('Seaborn version:    ' + sns.__version__)\n",
    "print('Matplotlib version: ' + matplotlib.__version__)\n",
    "print('PIL version:        ' + Image.__version__)\n",
    "print('TensorFlow version: ' + tf.__version__)\n",
    "print('Keras version:      ' + keras.__version__)\n",
    "print('Scikit-learn version: ' + sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNAzJi1c9WAX"
   },
   "source": [
    "Load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TM1qn2DKtjR6"
   },
   "outputs": [],
   "source": [
    "# Load and unzip the dataset\n",
    "\n",
    "# The data.zip is loaded from https://drive.google.com/drive/folders/1sajIcvGxBemqK_YIHFoY28EyV1Su_b5M?usp=drive_link\n",
    "zip_path = 'data.zip' \n",
    "extract_folder = 'waste_dataset' \n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_folder)\n",
    "\n",
    "print(f\"Dataset extracted to '{extract_folder}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDp_EWxVOhUu"
   },
   "source": [
    "## **2. Data Preparation** <font color=red> [25 marks] </font><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7Ac8VxvjWnw"
   },
   "source": [
    "### **2.1 Load and Preprocess Images** <font color=red> [8 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghmtINrMXDMy"
   },
   "source": [
    "Let us create a function to load the images first. We can then directly use this function while loading images of the different categories to load and crop them in a single step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZQ1UZNfQCWX"
   },
   "source": [
    "#### **2.1.1** <font color=red> [3 marks] </font><br>\n",
    "Create a function to load the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y6klNk9rcAtr"
   },
   "outputs": [],
   "source": [
    "# Create a function to load the raw images\n",
    "def load_images(folder_name, target_size=(64, 64)):\n",
    "    images = []\n",
    "    labels = []\n",
    "    # sub_folder_names = os.listdir(folder_name)\n",
    "    sub_folder_names = [name for name in os.listdir(folder_name) if name != 'Other']\n",
    "\n",
    "    for sub_folder_name in sub_folder_names:\n",
    "        sub_folder_path = os.path.join(folder_name, sub_folder_name)\n",
    "\n",
    "        file_names = os.listdir(sub_folder_path)\n",
    "\n",
    "        for file_name in filter(lambda k: '.png' in k, file_names):\n",
    "            file_path = os.path.join(sub_folder_path, file_name)\n",
    "\n",
    "            try:\n",
    "                file = Image.open(file_path).convert(mode='RGB')\n",
    "                file = file.resize(target_size)\n",
    "\n",
    "                normalized_image_array = np.array(file, dtype=np.float32) / 255.0\n",
    "                \n",
    "                images.append(normalized_image_array)\n",
    "                labels.append(sub_folder_name)\n",
    "            except Exception as ex:\n",
    "                print(f\"Error loading image {file_path}: {ex}\")\n",
    "    \n",
    "    return np.array(images), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J01VQrLhQsxx"
   },
   "source": [
    "#### **2.1.2** <font color=red> [5 marks] </font><br>\n",
    "Load images and labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_C9Oo0PTtYLf"
   },
   "source": [
    "Load the images from the dataset directory. Labels of images are present in the subdirectories.\n",
    "\n",
    "Verify if the images and labels are loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zm2zlZbmamzy"
   },
   "outputs": [],
   "source": [
    "# Get the images and their labels\n",
    "extract_folder_data = os.path.join(extract_folder, 'data')\n",
    "\n",
    "X, y = load_images(extract_folder_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of images loaded: {len(X)}\")\n",
    "print(f\"Number of labels: {len(y)}\")\n",
    "print(f\"Unique labels: {np.unique(y)}\")\n",
    "\n",
    "# Quick Observation\n",
    "# ------------------------\n",
    "# 1. There are 7625 images loaded and 7 unique labels\n",
    "# ------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "26Is-EwKuyGf"
   },
   "source": [
    "Perform any operations, if needed, on the images and labels to get them into the desired format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I64rs77bkAYk"
   },
   "source": [
    "### **2.2 Data Visualisation** <font color=red> [9 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCAepbyAQdI2"
   },
   "source": [
    "#### **2.2.1** <font color=red> [3 marks] </font><br>\n",
    "Create a bar plot to display the class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise Data Distribution\n",
    "label_counts = pd.Series(y).value_counts()\n",
    "\n",
    "sns.barplot(x=label_counts.index, y=label_counts.values)\n",
    "plt.title('Class Distribution of Waste Categories')\n",
    "plt.xlabel('Waste Category')\n",
    "plt.ylabel('Number of Images')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zNWsPfTzRh7x"
   },
   "source": [
    "#### **2.2.2** <font color=red> [3 marks] </font><br>\n",
    "Visualise some sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "37yXZzfLyOWt"
   },
   "outputs": [],
   "source": [
    "# Visualise Sample Images (across different labels)\n",
    "import random\n",
    "\n",
    "combined_data = list(zip(X, y))\n",
    "unique_labels = np.unique(y)\n",
    "sample_images = [random.choice([img for img, label in combined_data if label == l]) for l in unique_labels]\n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "\n",
    "for i, img in enumerate(sample_images):\n",
    "    plt.subplot(2, (len(sample_images) + 1) // 2, i + 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(unique_labels[i])\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IrxdFzNigaYG"
   },
   "source": [
    "#### **2.2.3** <font color=red> [3 marks] </font><br>\n",
    "Based on the smallest and largest image dimensions, resize the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EyVvjNXqgIGe"
   },
   "outputs": [],
   "source": [
    "# Find the smallest and largest image dimensions from the data set\n",
    "\n",
    "# 1. I have already resized and normalized the images. Hence showing the image size of hte sample imageset.\n",
    "\n",
    "for label, img in zip(unique_labels, sample_images):\n",
    "    h, w = img.shape[:2]\n",
    "    print(f\"Label: {label}, Height: {h}, Width: {w}\")\n",
    "\n",
    "# Quick Observation\n",
    "# ------------------------\n",
    "# 1. The height and width of the images are 64.\n",
    "#    As you can notice hte signature of function load_images(folder_name, target_size=(64, 64)).\n",
    "#    The attribute target_size is defaulted to 64, 64\n",
    "#    Hence the images are resized before being loaded into the collection.\n",
    "# ------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lCB8uOckR5li"
   },
   "source": [
    "### **2.3 Encoding the classes** <font color=red> [3 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZdC4dpTWt9eo"
   },
   "source": [
    "There are seven classes present in the data.\n",
    "\n",
    "We have extracted the images and their labels, and visualised their distribution. Now, we need to perform encoding on the labels. Encode the labels suitably."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Nwd0Ztvkf7K"
   },
   "source": [
    "####**2.3.1** <font color=red> [3 marks] </font><br>\n",
    "Encode the target class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qkyXDQN-660s"
   },
   "outputs": [],
   "source": [
    "# Encode the labels suitably\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "encoded_labels = label_encoder.fit_transform(y)\n",
    "\n",
    "for id, label in enumerate(label_encoder.classes_):\n",
    "    print(f\"{id}: {label}\")\n",
    "\n",
    "print(\"Encoded collection:\", encoded_labels)\n",
    "print(\"Actual collection:\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNBM4hsuSaoj"
   },
   "source": [
    "### **2.4 Data Splitting** <font color=red> [5 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0xw-Qlh29cZ"
   },
   "source": [
    "#### **2.4.1** <font color=red> [5 marks] </font><br>\n",
    "Split the dataset into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TErpx_JOkwjO"
   },
   "outputs": [],
   "source": [
    "# Assign specified parts of the dataset to train and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, encoded_labels, test_size=0.3, stratify=encoded_labels, random_state=100)\n",
    "\n",
    "print(f\"Image collection training set: {X_train.shape}\")\n",
    "print(f\"Image collection validation set: {X_val.shape}\")\n",
    "\n",
    "print(f\"Image labels training set: {y_train.shape}\")\n",
    "print(f\"Image labels validation set: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_train, counts_train = np.unique(y_train, return_counts=True)\n",
    "unique_val, counts_val = np.unique(y_val, return_counts=True)\n",
    "\n",
    "print(\"Training set class distribution:\", dict(zip(unique_train, counts_train)))\n",
    "print(\"Validation set class distribution:\", dict(zip(unique_val, counts_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mILXPeY-X-zP"
   },
   "source": [
    "## **3. Model Building and Evaluation** <font color=red> [20 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4E0afHwy5M_i"
   },
   "source": [
    "### **3.1 Model building and training** <font color=red> [15 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsu8K3tL5a5Q"
   },
   "source": [
    "#### **3.1.1** <font color=red> [10 marks] </font><br>\n",
    "Build and compile the model. Use 3 convolutional layers. Add suitable normalisation, dropout, and fully connected layers to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "awW9V2lmMK_d"
   },
   "source": [
    "Test out different configurations and report the results in conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oD7-2EXdz_Cl"
   },
   "outputs": [],
   "source": [
    "# Build and compile the model\n",
    "\n",
    "# We'll use sequential type of model. In this neural layers are added in a linear stack, one after the other\n",
    "# In our case we'll have an input layer, 3 convolutional layers and an output layer\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Conv2D is used for applying filters and detecting patterns\n",
    "# MaxPooling2D is used to impliment the pooling concept ie., get a 2 x 2 region and get the max value\n",
    "# Flatten is used to convert 2D feature maps to single dimension array, which can be fed into dense layers. 6 x 6 pixel image to an array of len 36\n",
    "# Dense takes the flatten input from each neuron, applies and activation function and outputs probabilities for each class.\n",
    "# Dropout drops a fraction of the neurons during training, helps prevent overfitting.\n",
    "# BatchNormalization helps in speeding up learning\n",
    "from tensorflow.keras.layers import (Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = sample_images[0].shape\n",
    "num_classes = len(unique_labels)\n",
    "\n",
    "# Reference: https://keras.io/api/models/sequential/\n",
    "# Sequential will group a linear stack of layers into a model. We can add multiple layers to the stack, one at a time.\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Input(shape=input_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional layer 1 added to the model\n",
    "# ----------------------------------------\n",
    "# Reference: https://keras.io/api/layers/convolution_layers/convolution2d/\n",
    "# 32 filters in the convolution\n",
    "# The convolutional window is 3,3\n",
    "# We'll use relu activation function\n",
    "# We'll use default stride 1, 1 ie., the filter will move one step at a time. Padding 'same' will add padding on all sides to retain the shape\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "\n",
    "# Reference: https://keras.io/api/layers/normalization_layers/batch_normalization/\n",
    "# Batch normalization applies a transformation that maintains the mean output close to 0 and the output standard deviation close to 1.\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Reference: https://keras.io/api/layers/pooling_layers/max_pooling2d/\n",
    "# By doing pooling, the system will go through 2 x 2 matrix and get the max value.\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Reference: https://keras.io/api/layers/regularization_layers/dropout/\n",
    "# Dropout is a regularization technique. In the following code we're turning off 25% of the neurons\n",
    "model.add(Dropout(0.25))\n",
    "# ----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional layer 2 added to the model\n",
    "# ----------------------------------------\n",
    "# The number of filters is increased to 64 for deeper learning than the basic understanding of the image\n",
    "# Also we're dropping 25% more neurons\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "# ----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional layer 3 added to the model\n",
    "# ----------------------------------------\n",
    "# The number of filters is increased to 128 for deeper learning than the basic understanding of the image\n",
    "# Also we're dropping 25% more neurons\n",
    "model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "# ----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected layers\n",
    "model.add(Flatten())  \n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://keras.io/api/models/model_training_apis/\n",
    "# Compile the model\n",
    "# Because labels are integer encoded, we use the loss function sparse_categorical_crossentropy https://keras.io/api/losses/probabilistic_losses/#sparsecategoricalcrossentropy-class\n",
    "model.compile(\n",
    "    # optimizer='adam',\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print shape details\n",
    "print(\"Layer-by-layer output shapes:\\n\")\n",
    "for i, layer in enumerate(model.layers):\n",
    "    print(layer)\n",
    "    print(layer.output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7t4duT1wX5wS"
   },
   "source": [
    "#### **3.1.2** <font color=red> [5 marks] </font><br>\n",
    "Train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcrEzo51Qj6w"
   },
   "source": [
    "Use appropriate metrics and callbacks as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M7Ut0BicH_I8"
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# Callback functions used - EarlyStopping and ModelCheckpoint\n",
    "# Reference: https://keras.io/api/callbacks/early_stopping/\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Reference: https://keras.io/api/callbacks/model_checkpoint/\n",
    "checkpoint_filepath = '/tmp/ckpt/checkpoint.weights.h5'\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "# Introducing a new callback funciton after the first observation - ReduceLROnPlateau\n",
    "# Reference: https://keras.io/api/callbacks/reduce_lr_on_plateau/\n",
    "reduce_lr_callback = ReduceLROnPlateau(\n",
    "    monitor='val_loss',    # or 'val_accuracy'\n",
    "    factor=0.5,            # reduce by half\n",
    "    patience=2,            # wait 2 epochs before reducing\n",
    "    min_lr=1e-6,           # don't go below this\n",
    "    verbose=1              # logs when LR is reduced\n",
    ")\n",
    "\n",
    "# Reference: https://keras.io/api/models/model_training_apis/#fit-method\n",
    "history = model.fit(\n",
    "    X_train, \n",
    "    encoded_labels, \n",
    "    batch_size=32, \n",
    "    epochs=10, \n",
    "    validation_split=0.3,\n",
    "    callbacks=[early_stopping_callback, model_checkpoint_callback, reduce_lr_callback]\n",
    ")\n",
    "\n",
    "# Quick Observations - Iteration 1:\n",
    "#----------------------------------\n",
    "# Training Accuracy is imploving by a small margin\n",
    "# Loss is gradually decreasing\n",
    "# Validation loss is huge! The training set it overfitting\n",
    "#\n",
    "# Epoch 1/10━━━━━━━━━━━━━━━━━━━━ - accuracy: 0.1639 - loss: 2.7129 - val_accuracy: 0.0000e+00 - val_loss: 22.4157\n",
    "# Epoch 2/10━━━━━━━━━━━━━━━━━━━━ - accuracy: 0.2416 - loss: 2.0073 - val_accuracy: 6.2422e-04 - val_loss: 30.6398\n",
    "# Epoch 3/10━━━━━━━━━━━━━━━━━━━━ - accuracy: 0.2680 - loss: 1.9028 - val_accuracy: 6.2422e-04 - val_loss: 35.1025\n",
    "# Epoch 4/10━━━━━━━━━━━━━━━━━━━━ - accuracy: 0.2704 - loss: 1.7594 - val_accuracy: 0.0062 - val_loss: 59.8457\n",
    "#----------------------------------\n",
    "\n",
    "# Quick Observations - Iteration 2:\n",
    "#----------------------------------\n",
    "# Overfitting has reduced\n",
    "# Learning rate reduction helped in reducing the validation loss\n",
    "# ReduceLROnPlateau kicked in after \n",
    "#----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWT-bIj9YVzh"
   },
   "source": [
    "### **3.2 Model Testing and Evaluation** <font color=red> [5 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjhU3i5v59d6"
   },
   "source": [
    "#### **3.2.1** <font color=red> [5 marks] </font><br>\n",
    "Evaluate the model on test dataset. Derive appropriate metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d_MtfUM_4y7j"
   },
   "outputs": [],
   "source": [
    "# Evaluate on the test set; display suitable metrics\n",
    "y_pred = model.predict(X_val)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(np.unique(np.argmax(y_pred, axis=1), return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred_classes)\n",
    "print(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_val, y_pred_classes)\n",
    "\n",
    "class_names = label_encoder.classes_\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names,\n",
    "            yticklabels=class_names,\n",
    "            annot_kws={\"size\": 10}) \n",
    "\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.title('Confusion Matrix', fontsize=14)\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "cm_df = pd.DataFrame(cm, index=[f\"True: {label}\" for label in class_names],\n",
    "                        columns=[f\"Pred: {label}\" for label in class_names])\n",
    "\n",
    "print(\"Confusion Matrix:\\n\")\n",
    "print(cm_df.to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "utro5JdHS0JM"
   },
   "source": [
    "## **4. Data Augmentation** <font color=red> [optional] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1T6QlG4eS4xi"
   },
   "source": [
    "#### **4.1 Create a Data Augmentation Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7AXlfuoa4jQV"
   },
   "source": [
    "##### **4.1.1**\n",
    "Define augmentation steps for the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vbHCwkX0dq0R"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.1,\n",
    "    rescale=1./255,\n",
    "    validation_split=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07i11vgMEmM2"
   },
   "source": [
    "Augment and resample the images.\n",
    "In case of class imbalance, you can also perform adequate undersampling on the majority class and augment those images to ensure consistency in the input datasets for both classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chvmgE2r4xPZ"
   },
   "source": [
    "Augment the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6-JBheeYFS8d"
   },
   "outputs": [],
   "source": [
    "# Create a function to augment the images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ddy1y1nPIlvM"
   },
   "outputs": [],
   "source": [
    "# Create the augmented training dataset\n",
    "train_augmented_data = train_datagen.flow(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    batch_size=32,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_augmented_data = train_datagen.flow(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    batch_size=32,\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZYekkw9TCvP"
   },
   "source": [
    "##### **4.1.2**\n",
    "\n",
    "Train the model on the new augmented dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tBcRbt57FEct"
   },
   "outputs": [],
   "source": [
    "# Train the model using augmented images\n",
    "history_augmented = model.fit(\n",
    "    train_augmented_data,\n",
    "    epochs=10,\n",
    "    validation_data=val_augmented_data,\n",
    "    callbacks=[early_stopping_callback, model_checkpoint_callback, reduce_lr_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "img_path = os.path.join('waste_dataset', 'data', 'Cardboard', 'file_12.png')\n",
    "\n",
    "img = image.load_img(img_path, target_size=(64,64))\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = img_array / 255.0\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "predictions = model.predict(img_array)[0]\n",
    "class_names = label_encoder.classes_\n",
    "\n",
    "print(\"Prediction probabilities:\")\n",
    "for i, prob in enumerate(predictions):\n",
    "    print(f\"{class_names[i]}: {prob * 100:.2f}%\")\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title(\"Test Image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFPuXAvHkJVz"
   },
   "source": [
    "## **5. Conclusions** <font color = red> [5 marks]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "33tWCHjpO5hH"
   },
   "source": [
    "#### **5.1 Conclude with outcomes and insights gained** <font color =red> [5 marks] </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3e1TLo2kWi0"
   },
   "source": [
    "* Report your findings about the data\n",
    "* Report model training results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1KW2JSuqBLb3DdmqZSAHtN2K0gX8C2HcV",
     "timestamp": 1740722968634
    },
    {
     "file_id": "1XXsgvgvRpr1OqI_K70kBWgfsI9bByK3r",
     "timestamp": 1738303842187
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
